1 Data Pre-processing and Exploration
1a Load a CSV dataset. Handle missing values, inconsistent
formatting, and outliers.
1b Load a dataset, calculate descriptive summary statistics,
create visualizations using different graphs, and identify
potential features and target variables
Note: Explore Univariate and Bivariate graphs (Matplotlib)
and Seaborn for visualization.
1c Create or Explore datasets to use all pre-processing routines
like label encoding, scaling, and binarization.

2 Testing Hypothesis
2a Implement and demonstrate the FIND-S algorithm for
finding the most specific hypothesis based on a given set of
training data samples. Read the training data cfrom a. CSV
file and generate the final specific hypothesis. (Create your
dataset)

3 Linear Models
3a Simple Linear Regression
Fit a linear regression model on a dataset. Interpret
coefficients, make predictions, and evaluate performance
using metrics like R-squared and MSE
3b Multiple Linear Regression
Extend linear regression to multiple features. Handle feature
selection and potential multicollinearity.
3c Regularized Linear Models (Ridge, Lasso, ElasticNet)
Implement regression variants like LASSO and Ridge on any
generated dataset.

4. Discriminative Models
4a Logistic Regression
Perform binary classification using logistic regression.
Calculate accuracy, precision, recall, and understand the
ROC curve.
4b Implement and demonstrate k-nearest Neighbor algorithm.
Read the training data from a .CSV file and build the model
to classify a test sample. Print both correct and wrong
predictions.
4c Build a decision tree classifier or regressor. Control
hyperparameters like tree depth to avoid overfitting.
Visualize the tree.
4d Implement a Support Vector Machine for any relevant
dataset.
4e Train a random forest ensemble. Experiment with the
number of trees and feature sampling. Compare performance
to a single decision tree.
4f Implement a gradient boosting machine (e.g., XGBoost).
Tune hyperparameters and explore feature importance.

5. Generative Models
5a Implement and demonstrate the working of a Naive Bayesian
classifier using a sample data set. Build the model to classify
a test sample.
5b Implement Hidden Markov Models using hmmlearn

6. Probabilistic Models
Implement Gaussian Mixture Models for density estimation
and unsupervised clustering

7. Model Evaluation and Hyperparameter Tuning
7a Implement cross-validation techniques (k-fold, stratified, etc.)
for robust model evaluation
7b Systematically explore combinations of hyperparameters to
optimize model performance.(use grid and randomized search)

8. Bayesian Learning
Implement Bayesian Learning using inferences

9. Deep Generative Models
Set up a generator network to produce samples and a
discriminator network to distinguish between real and
generated data. (Use a simple small dataset)

10. Develop an API to deploy your model and perform predictions
